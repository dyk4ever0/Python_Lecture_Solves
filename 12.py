# -*- coding: utf-8 -*-
"""12

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o07oeJ-w92vP5Cd2JT0ILAU-E_OpsgY8
"""

import numpy as np
import pandas as pd
from sklearn import linear_model
from sklearn import model_selection
from sklearn import preprocessing
from matplotlib import pyplot as plt

df = pd.read_csv('121.csv')
print(df.shape)

#make plot
plt.scatter(df.index1, df.index2)
plt.xlabel('index1')
plt.ylabel('index2')
plt.show()

#partitioning
x = df.loc[:,['index1']]
y = df['index2']
print(x.shape)
print(y.shape)
#train
x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, random_state=42) #0.3

#train into linear regression model
regr = linear_model.LinearRegression()
regr.fit(x_train, y_train)
regr.score(x_train, y_train)

regr.coef_

#x_test 예측값 - y_test MSE
y_pred= regr.predict(x_test)
np.mean((y_pred-y_test)**2) #MSE

plt.scatter(x_test, y_test, color='black')
plt.plot(x_test,y_pred,color='blue',linewidth=1)
plt.xlabel('index1')
plt.ylabel('index2')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import csv

file=open('1221knn.csv', 'r', encoding='utf-8')
rdr=csv.reader(file)
next(rdr)
# dataset #11단계에서 하는 부분 복습
init_data=[]
category=[]
for line in rdr:
    init_data.append([int(line[0]), int(line[1])])
    category.append(int(line[2]))


target = [5, 3] #이선 데이터 = 출신문화권, 정치성향

# make dataset
ds = np.array(init_data)
size = len(ds)
class_target = np.tile(target, (size, 1))
class_category = np.array(category)



print('DATA SET')
for i in range(len(ds)):
    print('[%-2d, %-2d]\'s class : %d'%(ds[i][0], ds[i][1], class_category[i]))

# 거리(점과 점사이의 거리식을 제공)
def classify(dataset, class_target, class_category, k):
    diff = class_target - dataset
    sqdiff = diff ** 2
    row_sum = sqdiff.sum(axis=1)
    distance = np.sqrt(row_sum)
# 거리 = 차의 제곱의 합의 제곱근 (차이가 가장 짧은 경우 구하기)
    sortdist = distance.argsort()
    class_result = {}
    for i in range(k):
        c = class_category[sortdist[i]]
        class_result[c] = class_result.get(c, 0) + 1

    return class_result

k = 3 #주의군/감시군/위험군(스토리상)
class_result = classify(ds, class_target, class_category, k)

def classify_result(class_result):
    cate1 = cate2 = cate3 = 0
    global class_category, ds, target
    ds = np.append(ds, np.array([target]), axis=0)
    for c in class_result.keys():
        if c == 1:

            cate1 = class_result[c]
        elif c == 2:

            cate2 = class_result[c]
        else:

            cate3 = class_result[c]

    if cate1 > cate2 and cate1 > cate3:
        class_category = np.append(class_category, np.array([1]))
        result = 'classified into category 1'
    elif cate2 > cate3 and cate2 > cate3:
        class_category = np.append(class_category, np.array([2]))
        result = 'classified into category 2'
    else:
        class_category = np.append(class_category, np.array([3]))
        result = 'classified into category 3'

    return result

result = classify_result(class_result)
print(target, end=' is ')
print(result)

# visualize

def visualize(data_set, category):
    marker = 'o'
    color_list = ['red', 'green', 'blue']
    for i in range(len(data_set)):
        classified_category = category[i] - 1
        if i==len(data_set)-1:
            plt.scatter(data_set[i][0], data_set[i][1], marker='*', c=color_list[classified_category])

        else:
            plt.scatter(data_set[i][0], data_set[i][1], marker=marker, c=color_list[classified_category])
    plt.show()

visualize(ds, class_category)

import numpy as np
import matplotlib.pyplot as plt
import csv
import math

init_data=[]
file=open('1231.csv', 'r', encoding='utf-8')
rdr=csv.reader(file)
next(rdr)
for line in rdr:
    init_data.append([float(line[1]), float(line[2])])

data_set=np.array(init_data)
for data in data_set:
    plt.scatter(data[0], data[1], c='green')
plt.show()

def distance(A, B):
    return math.sqrt((A[0]-B[0])**2+(A[1]-B[1])**2)

def avg(templist):
    x_sum=y_sum=0
    for item in templist:
        x_sum+=item[0]
        y_sum+=item[1]
    return [x_sum/len(templist), y_sum/len(templist)]

class kmeans:
    def __init__(self, k, tol, max_iter):
        self.k=k
        self.tol=tol
        self.max_iter=max_iter
        self.centroids=[]
        self.classifications={}

    def fit(self, dataset):
        for i in range(self.k):
            self.centroids.append(dataset[i])

        for i in range(self.max_iter):
            self.classifications={}

            for i in range(self.k):
                self.classifications[i]=[]

            for data in dataset:
                distances=[distance(data, centroid) for centroid in self.centroids]
                classification = distances.index(min(distances))
                self.classifications[classification].append(data)

            for i in range(self.k):
                self.centroids[i]=avg(self.classifications[i])

model=kmeans(3, 0.001, 300)
model.fit(data_set)
color_list=['grey', 'purple', 'orange']

for key, value in model.classifications.items():
    plt.scatter(model.centroids[key][0], model.centroids[key][1], c='red', marker='+', s=50)
    for point in value:
        plt.scatter(point[0], point[1], c=color_list[key], marker='*')
plt.show()

import csv
import math
import matplotlib.pyplot as plt
CATEGORY_NUM=3
point_data=[]
category=[]
distance_list=[]

file=open('knn_data.csv', 'r', encoding='utf-8')
rdr=csv.reader(file)
next(rdr)
for line in rdr:
    point_data.append([int(line[0]), int(line[1])])
    category.append(int(line[2]))


def distance(A, B):
    return math.sqrt((A[0]-B[0])**2+(A[1]-B[1])**2)

def knn(target, point_data, category, k):
    #거리 구한 뒤 정렬
    for i in range(len(point_data)):
        distance_list.append([distance(target, point_data[i]), category[i]])
    distance_list.sort(key=lambda x:x[0])

    check=[0 for i in range(CATEGORY_NUM)]
    #k개 확인
    for i in range(k):
        #카테고리가 1,2,3 이므로 배열 인덱스 접근을 위해 -1
        check[distance_list[i][1]-1]+=1

    #인접한 k개의 카테고리 중 가장 많은 것으로 classify
    maxIndex=0
    for i in range(len(check)):
        if(check[maxIndex] < check[i]):
            maxIndex=i
    point_data.append(target)
    category.append(maxIndex+1)

def visualize(data_set, category):
    marker = 'o'
    color_list = ['red', 'green', 'blue']
    for i in range(len(data_set)):
        classified_category = category[i] - 1
        if i==len(data_set)-1:
            plt.scatter(data_set[i][0], data_set[i][1], marker='*', c=color_list[classified_category], s=[20*2**4])

        else:
            plt.scatter(data_set[i][0], data_set[i][1], marker=marker, c=color_list[classified_category])
    plt.show()

target=[2, 10]
knn(target, point_data, category, 3)
visualize(point_data, category)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras.datasets import imdb

(X_train, y_train), (X_test, y_test) = imdb.load_data()

num_classes = max(y_train) + 1

len_result = [len(s) for s in X_train]

word_to_index = imdb.get_word_index()
index_to_word={}
for key, value in word_to_index.items():
    index_to_word[value+3] = key

import re
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GRU, Embedding
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.models import load_model

vocab_size = 10000
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocab_size)

max_len = 500
X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

model = Sequential()
model.add(Embedding(vocab_size, 100))
model.add(GRU(128))
model.add(Dense(1, activation='sigmoid'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('GRU_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)

def sentiment_predict(new_sentence):
  new_sentence = re.sub('[^0-9a-zA-Z ]', '', new_sentence).lower()

  # 정수 인코딩
  encoded = []
  for word in new_sentence.split():
    try :
      if word_to_index[word] <= 10000:
        encoded.append(word_to_index[word]+3)
      else:
        encoded.append(2)
    except KeyError:
      encoded.append(2)

  pad_new = pad_sequences([encoded], maxlen = max_len) # 패딩
  score = float(loaded_model.predict(pad_new)) # 예측
  if(score > 0.5):
    print("{:.2f}% 확률로 긍정".format(score * 100))
  else:
    print("{:.2f}% 확률로 부정".format((1 - score) * 100))

temp_str = "This is a lovely story about a grown man discovering his true gifts in life and it left a big smile on my face. A very sweet and inventive film that will hopefully leave most of us adults with something to think about when the credits role."
temp_str2 = "I just didn't click with this movie at all. The score is really nice and the voice acting is great. The animation is good but I really disliked the character designs. They just looked so fake and plastic and smooth. The whole theme and message just rang hollow to me as well. It was just like 'be you' 'find something in life'. But that was it. It was so surface level. There was no nuance to it, the characters were just like I want to find something in life. That's so heavy handed to me and not Interesting. The story was also a mixed bag of plots. I couldn't roll my eyes hard enough when a certain plot shown up. It is the most over done, boring and infuriating plot that for some reason shows up in every other kids movie. It was just so boring and made me just disconnect from the film because up until then I was pretty engaged with it and the world that they had created then they threw it away for a god awful plot that no one likes.I also didn't find it emotional at all. I thought that this plot would have really resonated with me but it was just so hollow and I don't know, kind of dull."
temp_str3 = "The music is haunting and adds to the unnerving atmosphere as does some of the most effective sound editing and design of the year, never being obvious or cheap and providing a lot of eeriness. The writing flows well and never becomes corny or overly-wordy, sometimes even thought-provoking in particularly in what it has to say about grief and trauma."

sentiment_predict(temp_str)
sentiment_predict(temp_str2)
sentiment_predict(temp_str3)